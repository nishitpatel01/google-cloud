{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b990a00-8a5b-4447-add3-3c87e8af6b68",
   "metadata": {},
   "source": [
    "# Train an image object detection model using Vertex AI AutoML (using APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82e5fe-a7e3-416e-bf98-b33de1753d6e",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we'll use Vertex AI to train customer object detection model using image dataset. After training, we will create an endpoint deploy the model to an endpoint and deploy the model to that endpoint. Then we will use the deploymed model for online prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b18d3-5070-4001-a19f-cb9907a38c9e",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3d560-d8ea-4376-b737-798f6fee661b",
   "metadata": {},
   "source": [
    " - Images dataset with objects to identify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711195d-e8ca-4945-a4e1-461f5a548d37",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c122206-25d6-404b-ac37-05b11e599d75",
   "metadata": {},
   "source": [
    " - Setup resources\n",
    " - Image dataset creation\n",
    " - Import images in the dataset and create labels\n",
    " - Train custom AutoML models for object tdetection\n",
    " - Evaluate the trained model\n",
    " - Create and endpoint for deploy model \n",
    " - Perform online prediction on new data\n",
    " - Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17848e-5d57-4c53-b97b-f4f60b056757",
   "metadata": {},
   "source": [
    "#### Setup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9461ff-ff59-4b5d-b79e-cad0d1f56bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages to run the notebook\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import base64\n",
    "from typing import List, Union\n",
    "from IPython.display import Image\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import trainingjob, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214c27bf-6326-4056-b00e-641143102492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables for notebooks\n",
    "\n",
    "LOCATION = \"us-central1\" \n",
    "PROJECT_ID = \"nishitp-daml\"\n",
    "DATASET_ID = \"4715062101670887424\"\n",
    "EXPORT_DIRECTORY = \"gs://ds-ml-demos/packages-image-metadata/\"\n",
    "ANNOTATION_SET_ID = \"package-classification_iod\"\n",
    "FILTER = f\"labels.aiplatform.googleapis.com/annotation_set_name={ANNOTATION_SET_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71b9fbf-7f63-40d4-8f29-e306369af3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating helper functions for API calls\n",
    "\n",
    "def manage_package_dataset_metadata(method, endpoint, data, auth_token):\n",
    "    data = str(data)\n",
    "    headers = {'Content-type': 'application/json', \"Authorization\": f\"Bearer {auth_token}\"}\n",
    "    \n",
    "    if method == \"GET\":\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "    if method == \"POST\":\n",
    "        resp = requests.post(endpoint, data=data, headers=headers)\n",
    "    if method == \"DELETE\":\n",
    "        resp = requests.delete(endpoint, headers=headers)\n",
    "    \n",
    "    return(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88fbf5-8e3a-49c0-b20b-f75dd004a529",
   "metadata": {},
   "source": [
    "##### Let's create image dataset in the Vertex AI and import all images into our dataset\n",
    "\n",
    "From the GCP console left task list pane, find and click on `Vertex AI` and then click on `Dataset` option to create a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf061c-e09c-43f2-a5c3-68e71d47818a",
   "metadata": {},
   "source": [
    "![](files/images/dm-01-gcp-console-vertex-ai-image-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac311e9-43ca-4924-b87b-24e55566ffc7",
   "metadata": {},
   "source": [
    "Once you are in the dataset `Create dataset` page, provide a name for dataset. We name our dataset `package-detection-ds` for this demo. Next, select `IMAGE` as dataset type and click on `Image object detection` option. Let's keep region and other setting as default for now. Next, click on `CREATE` button to create a new dataset resoure in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2cf660-3bb5-4dbf-b3c9-4e8005f146f5",
   "metadata": {},
   "source": [
    "![](files/images/dm-02-gcp-console-vertex-ai-image-dataset-creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cef4f-d0d4-4176-88eb-cba1f1c340cc",
   "metadata": {},
   "source": [
    "When the dataset is created, you will click on the dataset and import the images for labeling and training the model. You have following options to import images from within Verte AI dataset:\n",
    "\n",
    "- Upload from your local machine (limit 500 per upload)\n",
    "- Upload import file from your local machine (this file will contains the path to images and other attributes)\n",
    "- Select & import images from Cloud storage bucket (provide gcs location of the file. either jsonl or csv format)\n",
    "\n",
    "For this demo, we will use the file that we already have in the cloud stoage bucket and import. To do this, click on `IMPORT` pane and select `Select import files form Cloud Storage` option and provide path to the file and click on `BROWSE` button to select the gcs storage bucket and file. Lastly, click on `CONTINUE` button to import the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0362db-43c8-4c8c-bcd3-8cd81ca8f70e",
   "metadata": {},
   "source": [
    "![](files/images/dm-03-gcp-console-vertex-ai-image-dataset-import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d770b3-b5f2-4091-b6ca-fef199fd03d0",
   "metadata": {},
   "source": [
    "Once, all the images have been imported (based on dataset size it could take a while to load everything), you can browse it from the `BROWSE` pane of the dataset. You'll be able to create new labels for images and create boundaries around the object that you are trying to detect in the images. For example in this demo, we are trying to identify what kind of packages was delivered at the door. The labels are as follows:\n",
    "\n",
    "- small_packages\n",
    "- medium_packages\n",
    "- big_packages\n",
    "- envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f12fa-7f84-4a39-b548-ca1fa5f52fc1",
   "metadata": {},
   "source": [
    "![](files/images/dm-03-gcp-console-vertex-ai-image-dataset-browse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cebe7f-acc6-442f-9cf6-8c01dc52f470",
   "metadata": {},
   "source": [
    "After labeling all the images in our dataset, we are now ready to train the model. To do this, we will be using Vertex AI APIs. First, will create a function that will take custom inputs and starts the `Vertex AI training` resource.\n",
    "\n",
    "Below are the function input description:\n",
    "\n",
    "- `project`: your gcp project id\n",
    "- `display_name`: vertex ai training resournce name in the console\n",
    "- `dataset_id`: dataset id from previous step (check console->vertex ai->datasets->ID)\n",
    "- `model_display_name`: trained model name shown in console\n",
    "- `location`: location of your image dataset\n",
    "- `api_endpoint`: gcp vertex ai api endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e883838f-062d-4ba4-a052-6c3205a12a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training automl function\n",
    "\n",
    "def train_package_size_detection(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    dataset_id: str,\n",
    "    model_display_name: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests. This only needs to be created once. \n",
    "    aip_client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\n",
    "    \n",
    "    training_inputs = trainingjob.definition.AutoMlImageObjectDetectionInputs(\n",
    "        model_type=\"CLOUD_HIGH_ACCURACY_1\", # goal is to train a model with high accuracy as opposed to low latency serving here\n",
    "        budget_milli_node_hours=20000,      # this is the minimum numbee of node hours requires to train object detection model\n",
    "        disable_early_stopping=False,\n",
    "    ).to_value()\n",
    "\n",
    "    training_pipeline = {\n",
    "        \"display_name\": display_name,\n",
    "        \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_object_detection_1.0.0.yaml\",\n",
    "        \"training_task_inputs\": training_inputs,\n",
    "        \"input_data_config\": {\"dataset_id\": dataset_id},\n",
    "        \"model_to_upload\": {\"display_name\": model_display_name},\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "    response = aip_client.create_training_pipeline(\n",
    "        parent=parent, training_pipeline=training_pipeline\n",
    "    )\n",
    "    print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fc781-fc27-4279-b944-7b928d7c8c14",
   "metadata": {},
   "source": [
    "Let's train the model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ac8adc-5f11-4f58-9761-308288209afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/474014863033/locations/us-central1/trainingPipelines/554086790189809664\"\n",
      "display_name: \"package_size_detection_model_v1\"\n",
      "input_data_config {\n",
      "  dataset_id: \"4715062101670887424\"\n",
      "}\n",
      "training_task_definition: \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_object_detection_1.0.0.yaml\"\n",
      "training_task_inputs {\n",
      "  struct_value {\n",
      "    fields {\n",
      "      key: \"budgetMilliNodeHours\"\n",
      "      value {\n",
      "        string_value: \"20000\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"modelType\"\n",
      "      value {\n",
      "        string_value: \"CLOUD_HIGH_ACCURACY_1\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "model_to_upload {\n",
      "  display_name: \"package_size_detection_model_v1\"\n",
      "}\n",
      "state: PIPELINE_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1643244004\n",
      "  nanos: 584731000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1643244004\n",
      "  nanos: 584731000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the training pipeline \n",
    "train_package_size_detection(PROJECT_ID,  \n",
    "                             \"package_size_detection_model_v1\", \n",
    "                             DATASET_ID,\n",
    "                             \"package_size_detection_model_v1\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f6f53-f187-4041-8cb7-43e25341dd23",
   "metadata": {},
   "source": [
    "Once the model has been trained, you see that model resource in the Vertex AI `Training` page. Now we will evaulate the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137f8e5e-643d-4dbf-820c-f789323d3e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5589020189002825728'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get evaluation id\n",
    "import re\n",
    "model_client = aiplatform.gapic.ModelServiceClient(\n",
    "        client_options={\n",
    "            'api_endpoint':'us-central1-aiplatform.googleapis.com'\n",
    "            }\n",
    "        )\n",
    "\n",
    "evaluations = model_client.list_model_evaluations(parent=f'projects/{PROJECT_ID}/locations/{LOCATION}/models/2221017886154031104')  \n",
    "for val in evaluations:\n",
    "    res = re.search('evaluations/(.*)', val.name)\n",
    "    \n",
    "eval_id = res.group(1)\n",
    "eval_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f53fa70-0a6e-4a19-9c8d-5196b7fcaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to get aggregated evaluation metrices\n",
    "\n",
    "def evaluate_package_size_detection(\n",
    "    project: str,\n",
    "    model_id: str,\n",
    "    evaluation_id: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "   \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    name = client.model_evaluation_path(\n",
    "        project=project, location=location, model=model_id, evaluation=evaluation_id\n",
    "    )\n",
    "    response = client.get_model_evaluation(name=name)\n",
    "    print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496a99f-9367-4c0c-8ea0-56522aa40077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_package_size_detection(PROJECT_ID,  \n",
    "                             \"2221017886154031104\", #TODO: fetch this using api\n",
    "                             eval_id\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e62a2-bfe6-4846-b3db-7472617c7b35",
   "metadata": {},
   "source": [
    "Let's create an endpoint to deploy model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d144d9f-b36d-4588-abe1-82dcae7e3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint(\n",
    "    project: str, display_name: str, location: str,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=display_name, project=project, location=location,\n",
    "    )\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8c0177-0c6a-4af7-acff-e5de3a5c94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/474014863033/locations/us-central1/endpoints/609331751927414784/operations/2692660614433603584\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/474014863033/locations/us-central1/endpoints/609331751927414784\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/474014863033/locations/us-central1/endpoints/609331751927414784')\n",
      "package-object-detection-endpoint\n",
      "projects/474014863033/locations/us-central1/endpoints/609331751927414784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f296d6d0650> \n",
       "resource name: projects/474014863033/locations/us-central1/endpoints/609331751927414784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_endpoint(PROJECT_ID, \"package-object-detection-endpoint\", \"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39210a-32e3-4824-95b2-09380cc74423",
   "metadata": {},
   "source": [
    "Deploy model to this endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aa4d7-13c0-41d1-9961-85bbad923f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploying the model to endpoint\n",
    "\n",
    "def deploy_package_detection_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_name: str,  # fully qualified name: projects/project_number/locations/location_name/models/model_number\n",
    "    endpoint: Optional[aiplatform.Endpoint] = None,\n",
    "    deployed_model_display_name: Optional[str] = None,\n",
    "    traffic_percentage: Optional[int] = 0,\n",
    "    traffic_split: Optional[Dict[str, int]] = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    metadata: Optional[Sequence[Tuple[str, str]]] = (),\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "    model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        traffic_percentage=traffic_percentage,\n",
    "        traffic_split=traffic_split,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        metadata=metadata,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1822a1e-5203-4629-b2db-62196bbf921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_package_detection_model(PROJECT_ID, LOCATION, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffab18-a1d2-4a28-a5a0-b12085cb1e9c",
   "metadata": {},
   "source": [
    "Get predictions from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da197d3-b7e3-4c6b-84f4-80ab49e56737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_package_detection_predictions(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    filename: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    \n",
    "    with open(filename, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "    instance = predict.instance.ImageObjectDetectionPredictionInstance(\n",
    "        content=encoded_content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    \n",
    "    # See gs://google-cloud-aiplatform/schema/predict/params/image_object_detection_1.0.0.yaml for the format of the parameters.\n",
    "    parameters = predict.params.ImageObjectDetectionPredictionParams(\n",
    "        confidence_threshold=0.5, max_predictions=5,\n",
    "    ).to_value()\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/image_object_detection_1.0.0.yaml for the format of the predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\", dict(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0915c4-cf97-45d2-8a11-0a398cde727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_package_detection_predictions(PROJECT_ID, LOCATION, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473971cc-eb26-4a79-b3cb-2253f79aa017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
