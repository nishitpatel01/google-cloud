{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e90931b-6910-4fe5-835a-339ada1b287d",
   "metadata": {},
   "source": [
    "# Real-time anomaly detection using Timeseries Insights API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc791102-60b7-47db-891e-07b6db08e3f4",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebooks demostrate how to use Google Cloud's [Timeseries Insights API](https://cloud.google.com/timeseries-insights) for real time anomaly detection in time series data. This tutorial covers API dataset creation, querying for anomaly, append new data and deletion of unwanted API datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce47969-56d0-446b-8b0a-fe2a6515ca62",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- Timeseries dataset wth attributes that you want to detect anomaly on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d14c56-9c7b-4b06-9449-fcd3da42c1f0",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    " - Setup resources\n",
    " - Create dataset json file \n",
    " - Create and list all API dataset using json file in cloud storage bucket\n",
    " - Querying for anomaly in an API dataset\n",
    " - Append new data stream to an existing dataset\n",
    " - Consume the results for further analysis\n",
    " - Deleteting unwanted API dataset\n",
    " - Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49aaa48-eb76-45b2-8a55-2e496c55f4ad",
   "metadata": {},
   "source": [
    "#### Setup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495bb7d6-89dc-41ed-8399-235aec2fb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencises\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5517a8b3-fd33-43ff-b632-5e3f332e7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pnishit-mlai us-central1 <YOUR-BUCKET-NAME>\n"
     ]
    }
   ],
   "source": [
    "# Setup variables \n",
    "\n",
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"<YOUR-BUCKET-NAME>\"\n",
    "FILE_PATH = \"<local-path-to-your-processed-file>/data-file.json\"\n",
    "\n",
    "print(PROJECT_ID, REGION, BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c9fde-0cfb-4d90-9122-5d54a252a73f",
   "metadata": {},
   "source": [
    "#### Create and upoad dataset file to cloud storage bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b6b74-65f8-4064-b7c4-7adeb6a0c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket and upload the processed data file\n",
    "\n",
    "! gsutil mb -l $REGION -c standard gs://$BUCKET_NAME\n",
    "\n",
    "! gcloud alpha storage cp $FILE_PATH gs://$BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754d487-14be-4d63-a06d-5186db381215",
   "metadata": {},
   "source": [
    "#### Allowlist the API\n",
    "\n",
    "Since the API is in public preview at the moment, you'll need to allowlist Google cloud project you are working on. Once the API is GA, you don't need to perform this step and can skip it. \n",
    "\n",
    "Click [here](https://services.google.com/fb/forms/timeseries-insights-api-preview-registration/) to allowlist your Google cloud project. You won't be able to access the api till this step is completed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35949832-631b-4411-918d-afb968b9b322",
   "metadata": {},
   "source": [
    "#### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
    "\n",
    "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "1. **Click Create service account**.\n",
    "\n",
    "2. In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "3. In the **Grant this service account access to project** section, click the Role drop-down list. Select \"Other\" form the list, and scroll down and select **Timeseries Insights DataSet Owner**.\n",
    "\n",
    "4. Click Create. A JSON file that contains your key downloads to your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f35053-8c67-4475-86ec-cec254e8f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def query_ts(method, endpoint, data, auth_token):\n",
    "    data = str(data)\n",
    "    headers = {'Content-type': 'application/json', \"Authorization\": f\"Bearer {auth_token}\"}\n",
    "    \n",
    "    if method == \"GET\":\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "    if method == \"POST\":\n",
    "        resp = requests.post(endpoint, data=data, headers=headers)\n",
    "    if method == \"DELETE\":\n",
    "        resp = requests.delete(endpoint, headers=headers)\n",
    "\n",
    "    return(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8060f-1c03-41c9-a82a-1ad147305e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorize the service account\n",
    "\n",
    "!gcloud auth activate-service-account --key-file {key_file}\n",
    "token_array = !gcloud auth print-access-token \n",
    "token_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c29a4-9d08-438d-bf83-b8cd71ca1926",
   "metadata": {},
   "source": [
    "#### Create and list API dataset\n",
    "\n",
    "The first step in anomaly detection is to create dataset using the json data file from cloud storage bucket. The data file needs to be in the cloud storage bucket as during the dataset creation payload, it requires path to the file. Once you call the dataset create API method, tt can take a while to create a dataset depending on the dataset size. \n",
    "\n",
    "A list dataset method can be called to check the status of all datasets for Timeseries Insights API. All the datasets that have been loaded correctly will have status as `LOADED` and the ones that are currently being indexes will have status as `LOADING`. Note that a dataset can be queries for anomaly only after indexing is done and the dataset status changes to `LOADED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42e53b-e5b7-47a6-ab22-2a20ee6bc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset using API\n",
    "\n",
    "anomaly_dataset_payload = {\n",
    "    \"name\": \"anomaly-data\", \n",
    "    \"ttl\": \"3000000s\", # Set this only if using appending later. This tells API what records to discard. Events > ttl are discarded \n",
    "    \"dataNames\": [\n",
    "        \"measure\",\n",
    "        \"Humidity\",\n",
    "        \"Light\",\n",
    "        \"h2\",\n",
    "        \"temp\",\n",
    "    ],\n",
    "    \"dataSources\": [\n",
    "        {\"uri\": f\"gs://{BUCKET_NAME}/data-file.json\"} \n",
    "    ], \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950299c4-3125-4f89-92db-67f439b580c9",
   "metadata": {},
   "source": [
    "The above json objet is what is used to create the dataset in the Timeseries Insights API. The `name` attribute is just the name of API dataset. You can set any descriptive name you wish for it. `ttl` attribute stands for time to live (in seconds) which can be used to discard events when new data is being appended. This tells the API which older records to discard when creating the timeseries. The records with timestamp older than ttl values is discarded. Here ttl value is set to 3 million seconds which is roughly 35 days.\n",
    "\n",
    "Next `dataNames` attribute contains all the dimensions from your dataset that you want to index. Note that you can index all or subset of dimensions from your dataset. You can only query for anomalies using the dimensions that are indexed. In this case, all the dimensions from dataset have been used. Lastly, `dataSources` attribute contains the cloud storage bucket uri of json file that contains all your data.\n",
    "\n",
    "**Note: Once dataset has been created and indexes from given file, that file is no longer necessary for API to funtion. Adding new data to the file won't automatically create indexes on new data. You can only index new data using append method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e028-f88a-4648-bb87-20447f0e0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = query_ts(method=\"POST\", endpoint=ts_endpoint, data=anomaly_dataset_payload, auth_token=token_array[0])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a15571-71c2-40b6-beb6-330075b74acb",
   "metadata": {},
   "source": [
    "#### List dataset\n",
    "\n",
    "After running above command to create dataset, you can check the status using following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffb7a5-7271-467d-bee7-fad54af21f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdata = query_ts(method=\"GET\", endpoint=ts_endpoint, data=\"\", auth_token=token_array[0])\n",
    "listdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564d3ba-93a2-4126-9803-36a779a3e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Querying for anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fbac7-1935-4728-aa25-aa75ed467624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
