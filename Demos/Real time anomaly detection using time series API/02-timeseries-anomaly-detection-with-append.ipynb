{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28ad9ce-efc2-4120-a384-8d51041f28f6",
   "metadata": {},
   "source": [
    "# Timeseries Insights API - Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de07898-b971-4d23-9065-d24ab6f579ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook goes through both the preparation of the data, and then uses that data to test the Timeseries Insights API. In this notebook we will use the API to create a dataset, list all the datasets, fetch the time series of one of the dataset we created, query that dataset for an anomaly, and finally demonstrate how we would go about appending events in a streaming fashion with appendEvent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeef3ca-2dfc-4188-a137-7ff68df11993",
   "metadata": {},
   "source": [
    "## Preparation and visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfca471-1110-4309-b0e0-385aa558eb00",
   "metadata": {},
   "source": [
    "### Installing libraries and setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d38a0c-99c0-4351-a79b-604e8730b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip3 install --upgrade oauth2client \n",
    "!pip3 install pandas_bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a66f00-6d3c-42b4-bbbc-edf68a75dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fea95-53e7-428c-b771-83e59e7dda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "PROJECT_ID=\"<project-id>\"\n",
    "\n",
    "#Key file for API access and TSI API endopoint\n",
    "key_file = '/home/jupyter/TSI-API/keyfile/key.json' # JSON file has key of service account for Vertex AI\n",
    "ts_endpoint =  f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388420b2-2405-417f-b27d-59d8c145ccd1",
   "metadata": {},
   "source": [
    "### Extract full data from BigQuery and visualize with Pandas for reference when calling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca6c3d-4885-40cf-8aec-3abea015a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling bokeh library where to place output of visualization\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pandas_bokeh.output_file(\"chart.html\")\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0453fbd-cf0b-404f-9ef9-9a7f6a77c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract full dataset from BigQuery (200,000+ rows)\n",
    "\n",
    "sql = \"\"\"\n",
    "  select FARM_FINGERPRINT(CONCAT(time, temp, Humidity, Light, h2_raw)) groupId, \n",
    "            FORMAT_TIMESTAMP(\"%Y-%m-%dT%X%Ez\", time, \"UTC\") eventTime, \n",
    "            temp, \n",
    "            Humidity, \n",
    "            Light, \n",
    "            h2_raw,\n",
    "            'LTHH' as measure\n",
    "        from `<project-id>.<dataset-id>.full_ts_data`\n",
    "\"\"\"\n",
    "df = client.query(sql).to_dataframe()\n",
    "# df = df.melt(id_vars=['groupId','eventTime'])\n",
    "df = df.sort_values(\"eventTime\" , ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db153da0-d980-40c8-99a5-bc8a32e30405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame with normalized values for temp and light\n",
    "\n",
    "normFrame = df.copy(deep = True)\n",
    "cols_to_norm = ['temp','Light', 'Humidity', 'h2_raw']\n",
    "normFrame[cols_to_norm] = normFrame[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0590d-687f-449d-8c92-217b8b45da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display and save chart\n",
    "\n",
    "startTime = pd.to_datetime(\"06/14/21\", infer_datetime_format=True) \n",
    "endTime =  pd.to_datetime(\"08/09/21\", infer_datetime_format=True) \n",
    "size = (1400,500)\n",
    "normFrame.plot_bokeh(x = 'eventTime' , \n",
    "               xlabel = 'time',\n",
    "               y = [ 'temp', \"h2_raw\", \"Humidity\", \"Light\"], \n",
    "               kind = 'line', \n",
    "               figsize = size,\n",
    "               xlim = [startTime, endTime],\n",
    "               title=\"both (normalized)\"\n",
    "              )\n",
    "\n",
    "pandas_bokeh.output_file(\"chart.html\")\n",
    "df.plot_bokeh(x = 'eventTime' , \n",
    "               xlabel = 'time',\n",
    "               y = [\"Light\"], \n",
    "               kind = 'line', \n",
    "               figsize = size,\n",
    "               xlim = [startTime, endTime],\n",
    "               title=\"light\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ea677-b68b-4bc6-8bed-7f6abb05e23f",
   "metadata": {},
   "source": [
    "### Read data from BQ and visualize head to show the data converted to JSON for the API\n",
    "The data was converted into this format so that it could easily be made into a JSON file for dataset upload. This just displays how the data looks after some transformation not done in this notebook, with event timestamp (every 23 seconds), a hashed grouId (as required by API), and then the dimension data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049854cd-c66d-4ccc-9e36-f8cd239197c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read melted data from BQ table and prepare json for time series indight API dataset\n",
    "\n",
    "sql_out = \"\"\"\n",
    "with data as\n",
    "    (\n",
    "        select groupId, eventTime, STRUCT(variable as name, value as doubleVal) as dimensions \n",
    "        from (\n",
    "                select * from `<project-id>.<dataset-id>.anomaly_data`\n",
    "                order by eventTime, variable\n",
    "             )\n",
    "    )\n",
    "\n",
    "    SELECT eventTime, groupId, ARRAY_AGG(dimensions) AS dimensions FROM data GROUP BY eventTime, groupId \n",
    "\"\"\"\n",
    "df_out = client.query(sql_out).to_dataframe()\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e49f61-06fe-4494-b12e-ee04739a0c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Information on the JSON file used to upload to API\n",
    "In the examples below, we will be using the Cloud Stroage bucket item to upload our data to the sample dataset. This file contains a subset of our data, specifically the data between 06/14 to 06/17. The data was converted from BQ to this consolidated JSON format outside of this notebook.\n",
    "\n",
    "### A note on our slicing\n",
    "Due to the nature of the API, the dimension that is \"sliced\" on must be a string. Since our data is theoretically from one \"sensor\", we don't have multiple data points we want to slice on. As a result, in our JSON data, we have added a 5th dimension called \"measure\" that contains the same string for all values, simply as a filler so that we could query the time series API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b848657-7bc8-49cf-8841-99af4e842a23",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "Below we will establish some helper functions and an authorization token. We will then test creating a dataset, listing existing datasets, fetching a time series, querying a time series for anomalies, and then prep a request for streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383c353-6ea5-480d-aa53-d598d627f341",
   "metadata": {},
   "source": [
    "### Establish helper functions and extract authentication from Service Account key file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2c268-1d9f-4a7b-a079-5f1a940a8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads json file and returns request body\n",
    "\n",
    "def read_json_file(path):\n",
    "    with open(path) as json_file:\n",
    "        query = json.load(json_file)\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8d57e-92e2-4443-a96f-00d7321f4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with time series API\n",
    "\n",
    "def query_ts(method, endpoint, data, auth_token):\n",
    "    data = str(data)\n",
    "    headers = {'Content-type': 'application/json', \"Authorization\": f\"Bearer {auth_token}\"}\n",
    "    \n",
    "    if method == \"GET\":\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "    if method == \"POST\":\n",
    "        resp = requests.post(endpoint, data=data, headers=headers)\n",
    "    if method == \"DELETE\":\n",
    "        resp = requests.delete(endpoint, headers=headers)\n",
    "    #print(resp.content)\n",
    "    return(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7821c-8b79-4e5b-a3a0-c791314ff496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth activate-service-account --key-file {key_file}\n",
    "!gcloud auth print-access-token\n",
    "token_array = !gcloud auth print-access-token \n",
    "token = token_array[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51fc1e-6178-4b39-8792-9919e4ee369a",
   "metadata": {},
   "source": [
    "### 1. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa19175-96ce-4582-847b-c406a7f1a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset using API\n",
    "\n",
    "file_data = {\n",
    "    \"name\": \"data_test\",\n",
    "    \"dataNames\": [\n",
    "        \"measure\",\n",
    "        \"Humidity\",\n",
    "        \"Light\",\n",
    "        \"h2_raw\",\n",
    "        \"temp\",\n",
    "    ],\n",
    "    \"dataSources\": [\n",
    "        {\"uri\": \"gs://demo-ts-data/jbl-short-ts2.json\"} #sample of data in Cloud Storage JSON file\n",
    "    ]\n",
    "} \n",
    "res = query_ts(method=\"POST\", endpoint=ts_endpoint, data=file_data, auth_token=token)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5680ce1-c378-48cd-8c79-7b91bb4da0af",
   "metadata": {},
   "source": [
    "### 2. List datasets\n",
    "There are four other datasets we had created when testing, but here we should see the \"yaxin_test\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0a9a6-e195-4df6-9f10-218d167cdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdata = query_ts(method=\"GET\", endpoint=ts_endpoint, data=\"\", auth_token=token)\n",
    "listdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf98e2c-bb7c-4b35-8a25-66da26481b6c",
   "metadata": {},
   "source": [
    "### 3. Fetch Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7b7f3-b225-4364-a13d-42cfc3cc27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = {\n",
    "    \"pinnedDimensions\": [\n",
    "        {\n",
    "            \"name\":\"measure\",\n",
    "            \"stringVal\":\"LTTH\"\n",
    "        }\n",
    "        ],\n",
    "      \"timeInterval\": {\n",
    "        \"startTime\": \"2021-06-14T00:00:00Z\",\n",
    "        \"length\": \"864000s\"\n",
    "      },\n",
    "       \"granularity\": \"3600s\",\n",
    "       \"metric\": \"temp\"\n",
    "}\n",
    "\n",
    "# Fetch timeseries for inspection\n",
    "dataset_name = \"data_test\"\n",
    "fetch_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:fetchTimeseries'\n",
    "res = query_ts(method=\"POST\", endpoint=fetch_ds_endpt, data=file_data, auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325bd02-6bba-4d03-bacc-287e874e8afb",
   "metadata": {},
   "source": [
    "### 4. Querying the Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc55cb7-7b0b-4fba-9ed3-8ee808d7ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"detectionTime\": \"2021-06-17T16:40:00Z\",\n",
    "    \"slicingParams\": {\n",
    "        \"dimensionNames\": [\"measure\"]\n",
    "        },\n",
    "    \"timeseriesParams\": {\n",
    "        \"forecastHistory\": \"43200s\",\n",
    "        \"granularity\": \"450s\",\n",
    "        \"metric\": \"temp\"\n",
    "        },\n",
    "    \"forecastParams\": {\n",
    "        \"sensitivity\": 0.9,\n",
    "        \"noiseThreshold\": 12.0,\n",
    "        \"seasonalityHint\": \"DAILY\"\n",
    "        },\n",
    "   \n",
    "    \"returnNonAnomalies\": \"true\",\n",
    "    \"returnTimeseries\": \"true\"\n",
    "}\n",
    "\n",
    "\n",
    "# get forecast\n",
    "dataset_name = \"data_test\"\n",
    "query_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:query'\n",
    "res = query_ts(method=\"POST\", endpoint=query_ds_endpt, data=request_body, auth_token=token)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d348fa-d2cc-4c08-b04e-0207fdd6002f",
   "metadata": {},
   "source": [
    "### 5. Testing the appendEvents functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0aa88-345c-4eb4-970c-ebedb72acdd2",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "Because the streaming data appended has to be recent, we needed to perform some operations on our data in order to make the timestamps more recent. To do this we will take the data from a JSON file with dates beyond our created dataset which contains 06/14 - 06/17. We will then take the data immediately following it (data from 06/18 and 06/19) and convert that subset of data to the two days preceeeding the current date. This way we can then loop through and attempt to add this data to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1195e2-0042-4473-869a-87719b06e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all data from json, in order to itterate over it\n",
    "events = []\n",
    "count = 0\n",
    "data = 1\n",
    "\n",
    "#reading events from json file, and appending to evetns List\n",
    "with open('data/jbl-full-ts2.json', 'r') as myfile:\n",
    "    try:\n",
    "        while data:\n",
    "            data = myfile.readline()\n",
    "            json_load = json.loads(data)\n",
    "            if json_load['eventTime'] > \"2021-06-18T00:00:04+00:00\" and json_load['eventTime'] < \"2021-06-20T00:00:04+00:00\":\n",
    "                events.append(json_load)\n",
    "                count += 1\n",
    "    # TODO: error at end of json file, seems to be formatting\n",
    "    except:\n",
    "        print(\"met error (TODO). Current Count: \" + str(count))  # count for reference later\n",
    "\n",
    "# replace dates extracted to more current dates based on today's date\n",
    "\n",
    "for event in events:\n",
    "    event['eventTime'] = event['eventTime'].replace('2021-06-18', '2022-02-21')\n",
    "    event['eventTime'] = event['eventTime'].replace('2021-06-19', '2022-02-22')\n",
    "    event['groupId'] = event['groupId'][:-1]\n",
    "events[0]['eventTime'] # Check to make sure dates have been updated in event array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35df3e-a56e-4231-b867-7f6e26dc27ac",
   "metadata": {},
   "source": [
    "#### Send requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7ce75-9bdc-4928-849c-112d056deb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_endpoint = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:appendEvents'\n",
    "\n",
    "#body containing streamed events\n",
    "request_body = {\n",
    "   \"events\":[]\n",
    "}\n",
    "\n",
    "#choose number of appends - any number greator than 1 appends all events:\n",
    "appends = 1 # use this variable to toggle between 0 and >0 for uploading one or several events\n",
    "\n",
    "#TODO sleep 3 sec\n",
    "if appends > 0:\n",
    "    # iterating though 100 events and appending to data set\n",
    "    for event in events[0:100]: \n",
    "        request_body['events'] = [event]\n",
    "        res = query_ts('POST', url_endpoint, request_body, token)\n",
    "        time.sleep(1) # sleep to make sure not too many request are sent to API at once\n",
    "else:\n",
    "    #testing one append\n",
    "    request_body['events'] = events[0]\n",
    "    res = query_ts('POST', url_endpoint, request_body, token)\n",
    "        \n",
    "# printing result of last api call       \n",
    "print(json.dumps(res, indent=4))\n",
    "print(json.dumps(request_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ca976-c2fb-4584-a130-119c89307e3b",
   "metadata": {},
   "source": [
    "#### Comment on append response\n",
    "As seen above in the output, the response of a \"successful\" event appending is currently just an empty response. We think it would be helpful for deubugging purposes **if the response would return some sort of \"success\" message**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50a11e-ce64-4da3-b77f-6866298add63",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check if event(s) were appended\n",
    "This can be done by listing the datasets and checking if the num_items_examined value on the \"yaxin_test\" dataset is greater than 6968, which is what it was when we initally created it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98479b03-6fab-46d5-b0c2-4becacfaa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdata = query_ts(method=\"GET\", endpoint=ts_endpoint, data=\"\", auth_token=token)\n",
    "listdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843cb6d-9eb0-4f24-a105-088645c38043",
   "metadata": {},
   "source": [
    "#### Querying range of appended events\n",
    "As another test of appendEvents, we are going to run a time series query on the range of data we attempted to append. (The most recently ran test appended data that is pseudo from 02/20 and 02/21). If we are able run a succesful query on that timeframe, then we know the data may have been appended, even tho it isn't showing up when the dataset is listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f200468-bb78-4378-91f4-4bffc52bdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"detectionTime\": \"2022-02-20T16:40:00Z\",\n",
    "    \"slicingParams\": {\n",
    "        \"dimensionNames\": [\"measure\"]\n",
    "        },\n",
    "    \"timeseriesParams\": {\n",
    "        \"forecastHistory\": \"100000s\",\n",
    "        \"granularity\": \"450s\",\n",
    "        \"metric\": \"temp\",\n",
    "        },\n",
    "    \"forecastParams\": {\n",
    "        \"sensitivity\": 0.9,\n",
    "        \"noiseThreshold\": 12.0,\n",
    "        \"seasonalityHint\": \"DAILY\"\n",
    "        },\n",
    "   \n",
    "    \"returnNonAnomalies\": \"true\",\n",
    "    \"returnTimeseries\": \"true\"\n",
    "}\n",
    "\n",
    "\n",
    "# get forecast\n",
    "dataset_name = \"data_test\"\n",
    "query_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:query'\n",
    "res = query_ts(method=\"POST\", endpoint=query_ds_endpt, data=request_body, auth_token=token)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a853b04-333d-4b17-9c62-8863ec767552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 400,\n",
       "  'message': 'Request contains an invalid argument.',\n",
       "  'status': 'INVALID_ARGUMENT'}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_request_body = {\n",
    "    \"detectionTime\": \"2021-06-18T00:59:43Z\",\n",
    "    \"pinnedDimensions\": [{\"name\": \"measure\",  \"stringVal\": \"LTTH\"}],\n",
    "    \"timeseriesParams\": {\n",
    "        \"forecastHistory\": \"315400s\",\n",
    "        \"granularity\": \"3601s\",\n",
    "        \"metric\": \"temp\",\n",
    "        \"metricAggregationMethod\": \"AVERAGE\",\n",
    "        },\n",
    "    \"forecastParams\": {\n",
    "        \"sensitivity\": 0.9,\n",
    "        \"noiseThreshold\": 12.0,\n",
    "        \"seasonalityHint\": \"DAILY\"\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "# get forecast\n",
    "dataset_name = \"data_test\"\n",
    "query_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:evaluateSlice'\n",
    "res = query_ts(method=\"POST\", endpoint=query_ds_endpt, data=eval_request_body, auth_token=token)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
