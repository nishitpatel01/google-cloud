{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f81c9b-2c4a-4caf-a4d4-a524bedc7123",
   "metadata": {},
   "source": [
    "# Testing the Timeseries Insights API with Jabil's use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b186a32-0713-4577-a960-e97c3c9381f1",
   "metadata": {},
   "source": [
    "This notebook contains the code used to interact with the Timeseries Insights API with some publicly available data, and determine how to perform real-time anomaly detection with the data. \n",
    "\n",
    "Data was imported into BigQuery. From BiqQuery, we extracted the data, and transformed it in order to fit the Event data JSON format needed to interact with the Timeseries Insights API which can be found [here.](https://cloud.google.com/timeseries-insights/docs/reference/rest/v1/projects.datasets/appendEvents#Event)\n",
    "\n",
    "After converting the data into a JSONL file, this notebook will show how we went about creating the Timeseries dataset, streaming data to that dataset, and querying the dataset for an anomaly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793ea5a-cbac-4474-a0fd-a7bc0a4e5605",
   "metadata": {},
   "source": [
    "## Libraries, SA token authorization, and helper functions\n",
    "Importing necessary libraries, setting project varibale, and adding functions to help with interacting with Timeseries Insights API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8a161-5a1e-4631-9aeb-881a495c4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip3 install --upgrade oauth2client \n",
    "!pip3 install pandas_bokeh\n",
    "!pip install pyfarmhash\n",
    "!pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1707d4a2-206b-4e86-b326-795a9f31d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "469b4c51-68db-4c3b-ae07-ce21d7c71a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "PROJECT_ID=\"<project-id>\"\n",
    "\n",
    "#Key file for API access and TSI API endopoint\n",
    "key_file = '/home/jupyter/TSI-API/keyfile/key.json' # JSON file has key of service account for Vertex AI\n",
    "ts_endpoint =  f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5084f42-74ab-46a9-a27b-551187f70975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads json file and returns request body\n",
    "\n",
    "def read_json_file(path):\n",
    "    with open(path) as json_file:\n",
    "        query = json.load(json_file)\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc6d6cf0-23b5-4cd7-bfb2-a7776c65832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with time series API\n",
    "\n",
    "def query_ts(method, endpoint, data, auth_token):\n",
    "    data = str(data)\n",
    "    headers = {'Content-type': 'application/json', \"Authorization\": f\"Bearer {auth_token}\"}\n",
    "    \n",
    "    if method == \"GET\":\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "    if method == \"POST\":\n",
    "        resp = requests.post(endpoint, data=data, headers=headers)\n",
    "    if method == \"DELETE\":\n",
    "        resp = requests.delete(endpoint, headers=headers)\n",
    "    #print(resp.content)\n",
    "    return(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d18fd-e1b3-4641-bb93-9904c5275d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth activate-service-account --key-file {key_file}\n",
    "!gcloud auth print-access-token\n",
    "token_array = !gcloud auth print-access-token \n",
    "token = token_array[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e09ab-34fa-4428-8da4-6e5fd3e5c054",
   "metadata": {},
   "source": [
    "## Creation of bucket and list for test data\n",
    "We will now take the JSON from the full data, and extract all the data from 06/14-06/27. We will be changing the datest on all of this data from be equal to x number of days back from today's date, as if today was 06/28 (this is so that streaming can be properly tested - the Timeseries Insights API requires that appended events be within a certain window of today's date). So for instance, the data from 06/6 will be equal to two days back from today's date.\n",
    "\n",
    "The data from 06/14-06/24 (which will look like roughly data from 16 days ago to 6 days ago) will be store in a Cloud Storage bucket that will be used to create a dataset in batch form.\n",
    "\n",
    "The data from 06/24-06/27 will be put into a list so that it can be used to test the appendEvents functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb931e-c434-4450-95b8-aedbdae46758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data difference\n",
    "\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "days = datetime.timedelta(5)\n",
    "new = today - days\n",
    "print(today)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160a3edc-e84b-4731-9bb8-352fe7ac0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "\n",
    "batch_events = [] #list for items to go in Cloud Storage to generate dataset\n",
    "batch_full = [] # as sanity check, another dataset with all events done in batch will be done to check anomaly detection\n",
    "events = [] # list of items that will be appended via append events\n",
    "\n",
    "sampletoday = '2021-06-28'\n",
    "dateobjtoday = datetime.datetime.strptime(sampletoday, '%Y-%m-%d')\n",
    "todaysdate = dateobjtoday.date()\n",
    "\n",
    "\n",
    "#reading events from json file, and appending to events list and batch JSON file\n",
    "with open('/home/jupyter/TSI-API/jbl-full-ts2.json', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        json_load = json.loads(line)\n",
    "\n",
    "        if json_load['eventTime'] >= \"2021-06-14T00:00:04+00:00\" and json_load['eventTime'] < \"2021-06-23T00:00:04+00:00\":\n",
    "            \n",
    "            date_current = datetime.datetime.strptime(json_load['eventTime'], \"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "            shortform = date_current.date()\n",
    "            stringdate = shortform.strftime('%Y-%m-%d')\n",
    "            diff = todaysdate - shortform\n",
    "\n",
    "            real_date = date.today()\n",
    "            input_date = date.today() - diff\n",
    "            stringnewdate = input_date.strftime('%Y-%m-%d')\n",
    "  \n",
    "            json_load['eventTime'] = json_load['eventTime'].replace(stringdate, stringnewdate)\n",
    "\n",
    "            batch_events.append(json_load)\n",
    "            batch_full.append(json_load)\n",
    "            \n",
    "        elif json_load['eventTime'] >= \"2021-06-23T00:00:04+00:00\" and json_load['eventTime'] < \"2021-06-27T00:00:04+00:00\":\n",
    "            \n",
    "            date_current = datetime.datetime.strptime(json_load['eventTime'], \"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "            shortform = date_current.date()\n",
    " \n",
    "            stringdate = shortform.strftime('%Y-%m-%d')\n",
    "            diff = todaysdate - shortform\n",
    "      \n",
    "            real_date = date.today()\n",
    "            input_date = date.today() - diff\n",
    "            stringnewdate = input_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            json_load['eventTime'] = json_load['eventTime'].replace(stringdate, stringnewdate)\n",
    "\n",
    "            events.append(json_load)\n",
    "            batch_full.append(json_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "914c3671-7bb1-4051-aa3b-363183275952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14775\n",
      "25423\n",
      "40198\n"
     ]
    }
   ],
   "source": [
    "#print length of lists as sanity check\n",
    "\n",
    "print(len(events))\n",
    "print(len(batch_events))\n",
    "print(len(batch_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab09a5-8462-47e9-bdad-5234dd021f62",
   "metadata": {},
   "source": [
    "## Write batch data to bucket\n",
    "We will take the two lists that include both all the data points and the first half of data points, and store them in Cloud Storage so that we can reference these files in CS when creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c7df9fd-5503-4bb0-b47b-b1b2867ed859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write batch data to file and then export to Cloud Storage bucket\n",
    "with open('/home/jupyter/TSI-API/data/batchdata.json', 'w') as file_out:\n",
    "    for d in batch_events:\n",
    "        json.dump(d, file_out)\n",
    "        file_out.write('\\n')\n",
    "    \n",
    "#full batch data\n",
    "with open('/home/jupyter/TSI-API/data/batchdatafull.json', 'w') as file_out:\n",
    "    for d in batch_full:\n",
    "        json.dump(d, file_out)\n",
    "        file_out.write('\\n')\n",
    "    \n",
    "client = storage.Client(project='{PROJECT_ID}')\n",
    "bucket = client.get_bucket('tsi-data')\n",
    "blob = bucket.blob('batch1.json')\n",
    "with open('/home/jupyter/TSI-API/data/batchdata.json', 'rb') as file_out:\n",
    "    blob.upload_from_file(file_out)\n",
    "\n",
    "blob2 = bucket.blob('batch_full1.json')\n",
    "with open('/home/jupyter/TSI-API/data/batchdatafull.json', 'rb') as file_out:\n",
    "    blob2.upload_from_file(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11d1bc-5755-47b1-a42e-dfd77bba2c74",
   "metadata": {},
   "source": [
    "## Create new dataset from bucket data\n",
    "Now we will create a Timeseries Insights API dataset from our batch data that we just stored in a bucket in the file 'batch1.json'.\n",
    "To understand the reasoning behind the JSON payload format we have to use to send data to the API, please see the documentation [here.](https://cloud.google.com/timeseries-insights/docs/reference/rest/v1/projects.datasets#DataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f217d99-63d3-4869-9bbb-01eb599e26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset using API\n",
    "\n",
    "file_data = {\n",
    "    \"name\": \"test\", \n",
    "    \"ttl\": \"30000000s\",\n",
    "    \"dataNames\": [\n",
    "        \"measure\",\n",
    "        \"Humidity\",\n",
    "        \"Light\",\n",
    "        \"h2_raw\",\n",
    "        \"temp\",\n",
    "    ],\n",
    "    \"dataSources\": [\n",
    "        {\"uri\": \"gs://tsi-data/batch1.json\"} # sample of data in Cloud Storage JSON file\n",
    "    ], \n",
    "} \n",
    "res = query_ts(method=\"POST\", endpoint=ts_endpoint, data=file_data, auth_token=token)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcdd9b-df17-428c-a57b-b08eff316d43",
   "metadata": {},
   "source": [
    "### list data to check when creates\n",
    "Now we can call the datasets.list method of the API, which returns all the loaded datasets and those in the process of being loaded. This let's us know when our dataset is ready for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a392f-a31d-4254-91ad-d96e670e764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdata = query_ts(method=\"GET\", endpoint=ts_endpoint, data=\"\", auth_token=token)\n",
    "listdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ad7dc-a5f3-4785-9467-f2a98aefb0d9",
   "metadata": {},
   "source": [
    "## Append more recent events to dataset in streaming fashion\n",
    "This cell loops through a number of the items in our events list (all items after the events we ulpoaded in bath form) and then inidvidually appends those events to the dataset in order to simulate what would be streaming time. For this example, we are going to only append the first 100 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd5f96-460b-42c4-a8ae-1797484a6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_endpoint = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/partialbatch:appendEvents'\n",
    "\n",
    "#body containing streamed events\n",
    "request_body = {\n",
    "   \"events\":[]\n",
    "}\n",
    "\n",
    "#choose number of appends - any number greator than 1 appends all events:\n",
    "appends = 1 # use this variable to toggle between 0 and >0 for uploading one or several events\n",
    "i = 0\n",
    "\n",
    "#TODO sleep 3 sec\n",
    "if appends > 0:\n",
    "    # iterating though 100 events and appending to data set\n",
    "    for event in events[0:100] : \n",
    "        request_body['events'] = [event]\n",
    "        res = query_ts('POST', url_endpoint, request_body, token)\n",
    "        time.sleep(.1) # sleep to make sure not too many request are sent to API at once\n",
    "        i = i + 1\n",
    "        if i % 100 == 0:\n",
    "            print('another 100') #check to see progress\n",
    "else:\n",
    "    #testing one append\n",
    "    request_body['events'] = events[0]\n",
    "    res = query_ts('POST', url_endpoint, request_body, token)\n",
    "        \n",
    "# printing result of last api call    \n",
    "\n",
    "print('done')\n",
    "print(events[90]) # We are printing this event so that we can extract a timestamp to query the dataset later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdeecaf-85f0-4e93-be04-fad19a67fa82",
   "metadata": {},
   "source": [
    "## Query appeneded event\n",
    "We will now test out the query funcitonality of the API, which determines if the point we query is an anomaly or not, based on serveral timeseries parameters we specify to the API. The documentation on the format of the query can be found [here.](https://cloud.google.com/timeseries-insights/docs/reference/rest/v1/projects.datasets/query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fab62b45-d64e-4b13-b41d-7216934307ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/pnishit-mlai/datasets/test',\n",
       " 'anomalyDetectionResult': {'nonAnomalies': [{'dimensions': [{'name': 'measure',\n",
       "      'stringVal': 'LTTH'}],\n",
       "    'result': {'holdoutErrors': {'mdape': 0.018393232600039378,\n",
       "      'rmd': 0.3233111289100961},\n",
       "     'trainingErrors': {'mdape': 0.010853995807362472,\n",
       "      'rmd': 0.017651694827175206},\n",
       "     'forecastStats': {'density': '95', 'numAnomaliesInHoldout': 5},\n",
       "     'history': {'point': [{'time': '2022-04-19T12:30:00Z',\n",
       "        'value': 690.0871092569982},\n",
       "       {'time': '2022-04-19T12:37:30Z', 'value': 691.4770979135792},\n",
       "       {'time': '2022-04-19T12:45:00Z', 'value': 728.5865483626636},\n",
       "       {'time': '2022-04-19T12:52:30Z', 'value': 692.4363495380096},\n",
       "       {'time': '2022-04-19T13:00:00Z', 'value': 692.5467742864121},\n",
       "       {'time': '2022-04-19T13:07:30Z', 'value': 692.5157960164504},\n",
       "       {'time': '2022-04-19T13:15:00Z', 'value': 691.6372840701262},\n",
       "       {'time': '2022-04-19T13:22:30Z', 'value': 690.5362924507635},\n",
       "       {'time': '2022-04-19T13:30:00Z', 'value': 688.9960710683561},\n",
       "       {'time': '2022-04-19T13:37:30Z', 'value': 723.6159888640034},\n",
       "       {'time': '2022-04-19T13:45:00Z', 'value': 685.1660963048902},\n",
       "       {'time': '2022-04-19T13:52:30Z', 'value': 682.6266313617112},\n",
       "       {'time': '2022-04-19T14:00:00Z', 'value': 680.196450875996},\n",
       "       {'time': '2022-04-19T14:07:30Z', 'value': 677.2953077220153},\n",
       "       {'time': '2022-04-19T14:15:00Z', 'value': 670.4869914769132},\n",
       "       {'time': '2022-04-19T14:22:30Z', 'value': 663.4667885992201},\n",
       "       {'time': '2022-04-19T14:30:00Z', 'value': 695.0166989709431},\n",
       "       {'time': '2022-04-19T14:37:30Z', 'value': 658.7766330685079},\n",
       "       {'time': '2022-04-19T14:45:00Z', 'value': 657.3965412978655},\n",
       "       {'time': '2022-04-19T14:52:30Z', 'value': 655.8257931711893},\n",
       "       {'time': '2022-04-19T15:00:00Z', 'value': 653.5866456298571},\n",
       "       {'time': '2022-04-19T15:07:30Z', 'value': 685.8165116914747},\n",
       "       {'time': '2022-04-19T15:15:00Z', 'value': 649.6968295413217},\n",
       "       {'time': '2022-04-19T15:22:30Z', 'value': 648.0777469295792},\n",
       "       {'time': '2022-04-19T15:30:00Z', 'value': 646.7467472394555},\n",
       "       {'time': '2022-04-19T15:37:30Z', 'value': 644.9771546782164},\n",
       "       {'time': '2022-04-19T15:45:00Z', 'value': 643.2762937414552},\n",
       "       {'time': '2022-04-19T15:52:30Z', 'value': 675.6471098629337},\n",
       "       {'time': '2022-04-19T16:00:00Z', 'value': 640.7767115725878},\n",
       "       {'time': '2022-04-19T16:07:30Z', 'value': 639.8067961367058},\n",
       "       {'time': '2022-04-19T16:15:00Z', 'value': 638.9160238628705},\n",
       "       {'time': '2022-04-19T16:22:30Z', 'value': 638.5167588376362},\n",
       "       {'time': '2022-04-19T16:30:00Z', 'value': 636.8572026833102},\n",
       "       {'time': '2022-04-19T16:37:30Z', 'value': 636.1263979190578},\n",
       "       {'time': '2022-04-19T16:45:00Z', 'value': 670.3058364404205},\n",
       "       {'time': '2022-04-19T16:52:30Z', 'value': 638.4764696471254},\n",
       "       {'time': '2022-04-19T17:00:00Z', 'value': 639.8975172840704},\n",
       "       {'time': '2022-04-19T17:07:30Z', 'value': 641.1868843863195},\n",
       "       {'time': '2022-04-19T17:15:00Z', 'value': 642.0862453276463},\n",
       "       {'time': '2022-04-19T17:22:30Z', 'value': 643.4570185676688},\n",
       "       {'time': '2022-04-19T17:30:00Z', 'value': 678.7370850481446},\n",
       "       {'time': '2022-04-19T17:37:30Z', 'value': 646.2865852818428},\n",
       "       {'time': '2022-04-19T17:45:00Z', 'value': 646.6077188569205},\n",
       "       {'time': '2022-04-19T17:52:30Z', 'value': 646.367510535485},\n",
       "       {'time': '2022-04-19T18:00:00Z', 'value': 646.0971849011761},\n",
       "       {'time': '2022-04-19T18:07:30Z', 'value': 645.9063921930056},\n",
       "       {'time': '2022-04-19T18:15:00Z', 'value': 645.9774266742628},\n",
       "       {'time': '2022-04-19T18:22:30Z', 'value': 645.8666007896884},\n",
       "       {'time': '2022-04-19T18:30:00Z', 'value': 678.6867486392284},\n",
       "       {'time': '2022-04-19T18:37:30Z', 'value': 639.8458615616573},\n",
       "       {'time': '2022-04-19T18:45:00Z', 'value': 637.165518991756},\n",
       "       {'time': '2022-04-19T18:52:30Z', 'value': 637.6472706826522},\n",
       "       {'time': '2022-04-19T19:00:00Z', 'value': 638.4864524602204},\n",
       "       {'time': '2022-04-19T19:07:30Z', 'value': 638.665891863971},\n",
       "       {'time': '2022-04-19T19:15:00Z', 'value': 638.9275545010603},\n",
       "       {'time': '2022-04-19T19:22:30Z', 'value': 639.6970468165517},\n",
       "       {'time': '2022-04-19T19:30:00Z', 'value': 639.8677356978961},\n",
       "       {'time': '2022-04-19T19:37:30Z', 'value': 673.9866578753682},\n",
       "       {'time': '2022-04-19T19:45:00Z', 'value': 640.4071191259806},\n",
       "       {'time': '2022-04-19T19:52:30Z', 'value': 640.6164605004067},\n",
       "       {'time': '2022-04-19T20:00:00Z', 'value': 640.9668601304636},\n",
       "       {'time': '2022-04-19T20:07:30Z', 'value': 640.4364805911063},\n",
       "       {'time': '2022-04-19T20:15:00Z', 'value': 640.0681327919322},\n",
       "       {'time': '2022-04-19T20:22:30Z', 'value': 639.6579330334343},\n",
       "       {'time': '2022-04-19T20:30:00Z', 'value': 639.8879195176821},\n",
       "       {'time': '2022-04-19T20:37:30Z', 'value': 674.1359576463577},\n",
       "       {'time': '2022-04-19T20:45:00Z', 'value': 637.8465767441899},\n",
       "       {'time': '2022-04-19T20:52:30Z', 'value': 636.5565101426746},\n",
       "       {'time': '2022-04-19T21:00:00Z', 'value': 638.3673853306523},\n",
       "       {'time': '2022-04-19T21:07:30Z', 'value': 640.466745801785},\n",
       "       {'time': '2022-04-19T21:15:00Z', 'value': 642.0372253431672},\n",
       "       {'time': '2022-04-19T21:22:30Z', 'value': 643.5268272055158},\n",
       "       {'time': '2022-04-19T21:30:00Z', 'value': 644.5979134350335},\n",
       "       {'time': '2022-04-19T21:37:30Z', 'value': 645.7769221525493},\n",
       "       {'time': '2022-04-19T21:45:00Z', 'value': 679.4773078717758},\n",
       "       {'time': '2022-04-19T21:52:30Z', 'value': 644.6671167566591},\n",
       "       {'time': '2022-04-19T22:00:00Z', 'value': 644.0973381838576},\n",
       "       {'time': '2022-04-19T22:07:30Z', 'value': 643.1465095267578},\n",
       "       {'time': '2022-04-19T22:15:00Z', 'value': 642.7965852022833},\n",
       "       {'time': '2022-04-19T22:22:30Z', 'value': 642.2858865617768},\n",
       "       {'time': '2022-04-19T22:30:00Z', 'value': 641.8074015926788},\n",
       "       {'time': '2022-04-19T22:37:30Z', 'value': 675.4781010994601},\n",
       "       {'time': '2022-04-19T22:45:00Z', 'value': 641.3171535482372},\n",
       "       {'time': '2022-04-19T22:52:30Z', 'value': 638.0268589622566},\n",
       "       {'time': '2022-04-19T23:00:00Z', 'value': 633.2168898783259},\n",
       "       {'time': '2022-04-19T23:07:30Z', 'value': 634.1075618905178},\n",
       "       {'time': '2022-04-19T23:15:00Z', 'value': 637.5864136133112},\n",
       "       {'time': '2022-04-19T23:22:30Z', 'value': 674.2969706234657},\n",
       "       {'time': '2022-04-19T23:30:00Z', 'value': 643.6970655816265},\n",
       "       {'time': '2022-04-19T23:37:30Z', 'value': 646.7576156253826},\n",
       "       {'time': '2022-04-19T23:45:00Z', 'value': 650.1570033042777},\n",
       "       {'time': '2022-04-19T23:52:30Z', 'value': 205.87897490474512}]},\n",
       "     'forecast': {'point': [{'time': '2022-04-20T00:30:00Z',\n",
       "        'value': 645.0820337715933},\n",
       "       {'time': '2022-04-20T00:37:30Z', 'value': 645.0820337715933},\n",
       "       {'time': '2022-04-20T00:45:00Z', 'value': 645.0820337715933},\n",
       "       {'time': '2022-04-20T00:52:30Z', 'value': 645.0820337715933},\n",
       "       {'time': '2022-04-20T01:00:00Z', 'value': 645.0820337715933}]},\n",
       "     'detectionPointActual': 0,\n",
       "     'detectionPointForecast': 645.0820337715933,\n",
       "     'detectionPointForecastLowerBound': 595.9278766001283,\n",
       "     'detectionPointForecastUpperBound': 694.2361909430582,\n",
       "     'label': 'INSUFFICIENT_DATA'},\n",
       "    'status': {}}]}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"detectionTime\": \"2022-04-20T00:35:20Z\", # Input the date of a timestamp that was appneded to your dataset. We will query the timestamp at point event[90]\n",
    "    \"slicingParams\": {\n",
    "        \"dimensionNames\": [\"measure\"]\n",
    "        },\n",
    "    \"timeseriesParams\": {\n",
    "        \"forecastHistory\": \"43000s\",\n",
    "        \"granularity\": \"450s\",\n",
    "        \"metric\": \"temp\"\n",
    "        },\n",
    "    \"forecastParams\": {\n",
    "        \"sensitivity\": 0.90,\n",
    "        \"noiseThreshold\": 12.0,\n",
    "        \"seasonalityHint\": \"DAILY\"\n",
    "        },\n",
    "   \n",
    "    \"returnNonAnomalies\": \"true\",\n",
    "    \"returnTimeseries\": \"true\"\n",
    "}\n",
    "\n",
    "dataset = \"test\"\n",
    "\n",
    "# get forecast\n",
    "\n",
    "query_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset}:query'\n",
    "res = query_ts(method=\"POST\", endpoint=query_ds_endpt, data=request_body, auth_token=token)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff4a85-98a1-43fe-9524-521e30bb583c",
   "metadata": {},
   "source": [
    "## Finally, testing out the last API method, evaluateSlice\n",
    "This endpoint evaluates a specific \"slice\" of a dataset instead of performing anomaly detection on multiple. Our data is one-dimensional in nature, so we only have one \"slice\" to begin with. More information on the method can be found [here.](https://cloud.google.com/timeseries-insights/docs/reference/rest/v1/projects.datasets/evaluateSlice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b658fabf-0dff-4443-b903-a7bd544374f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = {\n",
    "    \"pinnedDimensions\": [\n",
    "        {\n",
    "            \"name\":\"measure\",\n",
    "            \"stringVal\":\"LTTH\"\n",
    "        }\n",
    "        ],\n",
    "    \"detectionTime\": \"2022-04-20T00:30:13Z\", #this time can be wherever you want to query\n",
    "    \"timeseriesParams\": {\n",
    "        \"forecastHistory\": \"50000s\",\n",
    "        \"granularity\": \"450s\",\n",
    "        \"metric\": \"temp\"\n",
    "        },\n",
    "    \"forecastParams\": {\n",
    "        \"sensitivity\": 0.90,\n",
    "        \"noiseThreshold\": 12.0,\n",
    "        \"seasonalityHint\": \"DAILY\"\n",
    "        }\n",
    "}\n",
    "\n",
    "# Fetch timeseries for inspection\n",
    "dataset_name = \"test\"\n",
    "fetch_ds_endpt = f'https://timeseriesinsights.googleapis.com/v1/projects/{PROJECT_ID}/datasets/{dataset_name}:evaluateSlice'\n",
    "res = query_ts(method=\"POST\", endpoint=fetch_ds_endpt, data=file_data, auth_token=token)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
